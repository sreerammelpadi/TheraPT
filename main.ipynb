{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DL Project\n",
        "\n",
        "## Transformer-based Empathetic Dialogue System\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OoQ_sfOaTBCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries"
      ],
      "metadata": {
        "id": "OJuQyRwGUQdj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0HselAHSrT_",
        "outputId": "b65042d6-9662-4f55-93ac-46a988514b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-03 16:12:54.221472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-03 16:12:54.398420: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import ast\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Therapeutic conversations dataset"
      ],
      "metadata": {
        "id": "MoNBANIBUTP0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etPmwVPwSrUA"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Viewing the dataset for its structure"
      ],
      "metadata": {
        "id": "YoZhl832UYel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkzeRvh9SrUA",
        "outputId": "60f9789b-0d55-470a-d4a2-a73b04ed37a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversations</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[{'from': 'human', 'value': \"I've been feeling...</td>\n",
              "      <td>identity_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[{'from': 'human', 'value': \"Hi, I'm feeling r...</td>\n",
              "      <td>identity_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[{'from': 'human', 'value': \"Hey, I hope you'r...</td>\n",
              "      <td>identity_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[{'from': 'human', 'value': \"I'm feeling reall...</td>\n",
              "      <td>identity_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[{'from': 'human', 'value': \"I'm feeling reall...</td>\n",
              "      <td>identity_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99081</th>\n",
              "      <td>[{'from': 'human', 'value': \"Hi Alex, I hope y...</td>\n",
              "      <td>identity_99465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99082</th>\n",
              "      <td>[{'from': 'human', 'value': \"I'm feeling reall...</td>\n",
              "      <td>identity_99466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99083</th>\n",
              "      <td>[{'from': 'human', 'value': \"I'm feeling regre...</td>\n",
              "      <td>identity_99467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99084</th>\n",
              "      <td>[{'from': 'human', 'value': \"Hi Alex, I'm feel...</td>\n",
              "      <td>identity_99468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99085</th>\n",
              "      <td>[{'from': 'human', 'value': \"I've been feeling...</td>\n",
              "      <td>identity_99469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99086 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           conversations              id\n",
              "0      [{'from': 'human', 'value': \"I've been feeling...      identity_0\n",
              "1      [{'from': 'human', 'value': \"Hi, I'm feeling r...      identity_1\n",
              "2      [{'from': 'human', 'value': \"Hey, I hope you'r...      identity_2\n",
              "3      [{'from': 'human', 'value': \"I'm feeling reall...      identity_3\n",
              "4      [{'from': 'human', 'value': \"I'm feeling reall...      identity_4\n",
              "...                                                  ...             ...\n",
              "99081  [{'from': 'human', 'value': \"Hi Alex, I hope y...  identity_99465\n",
              "99082  [{'from': 'human', 'value': \"I'm feeling reall...  identity_99466\n",
              "99083  [{'from': 'human', 'value': \"I'm feeling regre...  identity_99467\n",
              "99084  [{'from': 'human', 'value': \"Hi Alex, I'm feel...  identity_99468\n",
              "99085  [{'from': 'human', 'value': \"I've been feeling...  identity_99469\n",
              "\n",
              "[99086 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Viewing one data sample\n",
        "\n",
        "Each data sample shoes the conversation exchanges between 'human' and 'gpt'"
      ],
      "metadata": {
        "id": "nXRnpI-fUbxm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqrFFfljSrUA",
        "outputId": "c4428f24-e20c-40bd-c7d4-1fe10562d8e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'from': 'human', 'value': \"I've been feeling so sad and overwhelmed lately. Work has become such a massive source of stress for me.\"}\n",
            " {'from': 'gpt', 'value': \"Hey there, I'm here to listen and support you. It sounds like work has been really challenging lately. Can you tell me more about what's been going on?\"}\n",
            " {'from': 'human', 'value': \"I recently got a promotion at work, which I thought would be exciting. But the added responsibilities and pressure have just taken a toll on my mental health. It's been a really moving experience for me.\"}\n",
            " {'from': 'gpt', 'value': \"I can understand how it can be overwhelming when we're faced with higher expectations. It's okay to acknowledge your emotions and allow yourself to feel sad in this situation. It's an important part of the healing process. What specific challenges have you been facing at work?\"}\n",
            " {'from': 'human', 'value': \"Well, the workload has increased significantly, and I find it hard to maintain a work-life balance. I've been staying late at the office, and it feels like I'm constantly under a pile of never-ending tasks. It's just so hard to keep up, and it's impacting my overall well-being.\"}\n",
            " {'from': 'gpt', 'value': \"It sounds like you're dealing with a lot of pressure to perform and succeed. Remember, it's crucial to take care of yourself, both mentally and physically. A healthy work-life balance is essential. Have you tried any strategies to cope with the added workload?\"}\n",
            " {'from': 'human', 'value': \"I've been trying to prioritize my tasks and delegate whenever possible. I've also started practicing meditation during my breaks to help manage stress. But sometimes, it feels like no matter what I do, I can't catch a break. It's been a constant struggle.\"}\n",
            " {'from': 'gpt', 'value': \"It's great to hear that you're already implementing some helpful strategies. Remember, progress takes time, and it's okay to have setbacks. In addition to what you're already doing, I encourage you to also communicate with your supervisor or team about your workload and discuss possible solutions together.\"}\n",
            " {'from': 'human', 'value': \"You're right. I haven't really opened up about my struggles to my coworkers or supervisor. I guess I've been afraid of appearing weak or incapable. How can I approach this discussion without feeling vulnerable?\"}\n",
            " {'from': 'gpt', 'value': \"It's completely normal to feel that way, but remember, asking for support is a strength, not a weakness. Start by scheduling a conversation with your supervisor or a trusted colleague in a private and comfortable setting. Be honest about your challenges and express your willingness to find solutions together. Remember, you're not alone in this.\"}\n",
            " {'from': 'human', 'value': \"Thank you for your understanding and guidance. I appreciate the reminder that I don't have to face this alone. I'll gather my courage and initiate that conversation soon. I need to prioritize my well-being and find a healthier balance.\"}\n",
            " {'from': 'gpt', 'value': \"You're very welcome! I'm here to support you every step of the way. Taking care of yourself should always be a priority. Remember to be kind to yourself and celebrate your progress, no matter how small it may seem. You've got this!\"}]\n"
          ]
        }
      ],
      "source": [
        "print(df['conversations'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing dataset\n",
        "\n",
        "Dataset contains many characters that the model doesn't undertsnad, so we clean it as needed.\n",
        "\n",
        "We also convert the dataset from the dictionary format into a single string called \"conversation_text\" in order to train the model."
      ],
      "metadata": {
        "id": "I6DdljUPU7ia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gync7FL-SrUB",
        "outputId": "d4513187-d07d-48ef-da6c-84e21c7d2e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                       conversations          id  \\\n",
            "0  [{'from': 'human', 'value': \"I've been feeling...  identity_0   \n",
            "1  [{'from': 'human', 'value': \"Hi, I'm feeling r...  identity_1   \n",
            "2  [{'from': 'human', 'value': \"Hey, I hope you'r...  identity_2   \n",
            "3  [{'from': 'human', 'value': \"I'm feeling reall...  identity_3   \n",
            "4  [{'from': 'human', 'value': \"I'm feeling reall...  identity_4   \n",
            "\n",
            "                                                text  \n",
            "0  Human: I've been feeling so sad and overwhelme...  \n",
            "1  Human: Hi, I'm feeling really scared about som...  \n",
            "2  Human: Hey, I hope you're doing well. I wanted...  \n",
            "3  Human: I'm feeling really overwhelmed, and I d...  \n",
            "4  Human: I'm feeling really upset and confused a...  \n"
          ]
        }
      ],
      "source": [
        "def pre_process(conv_str):\n",
        "\n",
        "    conv_str_fixed = re.sub(r\"}\\s*{\", \"}, {\", conv_str)  # to insert a comma between each dictionary entries if not there.\n",
        "\n",
        "    conv_list = ast.literal_eval(conv_str_fixed)  # convert to list of dictionaries.\n",
        "\n",
        "    conversation_text = \"\"\n",
        "    for turn in conv_list:\n",
        "        speaker = turn.get(\"from\", \"unknown\").capitalize()\n",
        "        value = turn.get(\"value\", \"\")\n",
        "        conversation_text += f\"{speaker}: {value}\\n\"\n",
        "    return conversation_text.strip()\n",
        "\n",
        "df[\"text\"] = df[\"conversations\"].apply(pre_process)\n",
        "\n",
        "# View the new fomrat\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the dataset"
      ],
      "metadata": {
        "id": "QLEJMVXkV732"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2mUR6AVSrUB"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(df[[\"text\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "\n",
        "We have used the tokenizer from gpt2 model for this project."
      ],
      "metadata": {
        "id": "azhSKVxtWCnF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1d919080f3be463e963499e646f51fea",
            "8bc85fad00344db8bc90696ab712fc13",
            "37f36a72270242b8a5245317b887fe0c",
            "2f8312b4f90b47f1b96a2e35c1f96d54"
          ]
        },
        "id": "tpLvXATMSrUB",
        "outputId": "601724cb-0b3d-4f05-d96b-cc70890c5bf1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d919080f3be463e963499e646f51fea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bc85fad00344db8bc90696ab712fc13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37f36a72270242b8a5245317b887fe0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f8312b4f90b47f1b96a2e35c1f96d54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "if tokenizer.pad_token is None: # GPT-2 doesn't have a pad token by default; set it to the EOS token.\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_func(examples):\n",
        "    tokens = tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
        "\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # GPT-2 needs labels to compute loss, hence we add labels as same as input_ids.\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "AdfK9WPSSrUB",
        "outputId": "cf5379c7-eb1b-473b-9f27-da4999b539cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/99086 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(tokenize_func, batched=True, remove_columns=[\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlUSMob8SrUC"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using pre-trained GPT-2 model as the base model\n",
        "\n"
      ],
      "metadata": {
        "id": "wxZPG6Z4YUPs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Tt_2x6SrUC",
        "outputId": "52cf942d-345d-4edd-ec14-86a0b8c76719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /user/sreeramm/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /user/sreeramm/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned_therapy\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Let us Fine tune the model"
      ],
      "metadata": {
        "id": "EiacxJzMZiu1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBe2qjtMSrUC",
        "outputId": "6bbba19f-bff1-46ad-8a40-35ab4e6d22ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/transformers/4.21.1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 99086\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 37158\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37158' max='37158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37158/37158 1:13:38, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.777100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.712100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.654200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.627400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.591000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.578200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.551400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.541900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.512800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.506600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.497000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.481200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.462000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.461400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.446000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.451300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.431500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.423400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.424400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.417000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.408800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.411000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.401000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.391300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.397100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.386200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.383100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.382900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.371900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.373300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.360800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.352700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.350100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.364600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.363500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>1.354400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.337300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>1.342300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.345000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>1.329200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>1.339500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>1.329100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>1.318200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.326800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>1.318400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>1.327700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>1.308100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>1.306300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.310200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>1.310700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>1.310900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>1.298700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>1.302500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.288700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>1.297800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>1.290900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>1.305400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>1.281300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.301200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>1.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>1.296000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>1.293100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>1.281100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.280100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>1.283200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>1.270600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>1.269000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>1.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.264700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>1.274400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>1.261700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>1.277000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>1.264900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>1.274500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>1.265700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>1.254600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>1.251600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>1.278100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.264000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>1.262300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>1.246700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>1.253300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>1.264500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>1.260300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>1.249900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>1.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>1.242800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>1.255700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.246900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>1.250200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>1.254400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>1.254700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>1.233100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>1.238600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>1.237900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>1.237800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>1.235900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>1.245500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.234100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10100</td>\n",
              "      <td>1.235100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>1.229100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10300</td>\n",
              "      <td>1.234700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>1.231800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>1.227900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10600</td>\n",
              "      <td>1.232300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10700</td>\n",
              "      <td>1.237400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>1.232300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10900</td>\n",
              "      <td>1.236000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>1.227500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11100</td>\n",
              "      <td>1.222000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>1.216200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11300</td>\n",
              "      <td>1.223400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11400</td>\n",
              "      <td>1.213600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>1.232300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11600</td>\n",
              "      <td>1.212500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11700</td>\n",
              "      <td>1.219000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11800</td>\n",
              "      <td>1.213900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11900</td>\n",
              "      <td>1.223200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>1.219800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12100</td>\n",
              "      <td>1.224700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12200</td>\n",
              "      <td>1.205100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12300</td>\n",
              "      <td>1.212900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12400</td>\n",
              "      <td>1.213500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>1.194500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12600</td>\n",
              "      <td>1.184800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12700</td>\n",
              "      <td>1.198100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12800</td>\n",
              "      <td>1.196100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12900</td>\n",
              "      <td>1.200800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>1.193700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13100</td>\n",
              "      <td>1.185100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13200</td>\n",
              "      <td>1.189500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13300</td>\n",
              "      <td>1.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13400</td>\n",
              "      <td>1.193500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>1.194900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13600</td>\n",
              "      <td>1.189900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13700</td>\n",
              "      <td>1.193200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13800</td>\n",
              "      <td>1.181600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13900</td>\n",
              "      <td>1.200100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>1.188400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14100</td>\n",
              "      <td>1.182200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14200</td>\n",
              "      <td>1.185700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14300</td>\n",
              "      <td>1.178900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14400</td>\n",
              "      <td>1.181800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>1.192600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14600</td>\n",
              "      <td>1.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14700</td>\n",
              "      <td>1.179600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14800</td>\n",
              "      <td>1.184000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14900</td>\n",
              "      <td>1.181000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>1.179900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15100</td>\n",
              "      <td>1.175100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15200</td>\n",
              "      <td>1.192500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15300</td>\n",
              "      <td>1.179500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15400</td>\n",
              "      <td>1.184900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>1.184500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15600</td>\n",
              "      <td>1.178000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15700</td>\n",
              "      <td>1.178300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15800</td>\n",
              "      <td>1.179900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15900</td>\n",
              "      <td>1.173800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>1.186500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16100</td>\n",
              "      <td>1.162800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16200</td>\n",
              "      <td>1.176800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16300</td>\n",
              "      <td>1.176100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16400</td>\n",
              "      <td>1.168000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>1.163300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16600</td>\n",
              "      <td>1.172100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16700</td>\n",
              "      <td>1.174400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16800</td>\n",
              "      <td>1.174500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16900</td>\n",
              "      <td>1.166000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>1.168700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17100</td>\n",
              "      <td>1.168600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17200</td>\n",
              "      <td>1.166400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17300</td>\n",
              "      <td>1.177900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17400</td>\n",
              "      <td>1.169700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>1.169100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17600</td>\n",
              "      <td>1.161400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17700</td>\n",
              "      <td>1.165500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17800</td>\n",
              "      <td>1.171200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17900</td>\n",
              "      <td>1.176800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>1.162800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18100</td>\n",
              "      <td>1.169000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18200</td>\n",
              "      <td>1.171900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18300</td>\n",
              "      <td>1.167000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18400</td>\n",
              "      <td>1.169500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>1.174400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18600</td>\n",
              "      <td>1.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18700</td>\n",
              "      <td>1.164100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18800</td>\n",
              "      <td>1.176300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18900</td>\n",
              "      <td>1.165500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>1.160500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19100</td>\n",
              "      <td>1.165000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19200</td>\n",
              "      <td>1.160300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19300</td>\n",
              "      <td>1.170600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19400</td>\n",
              "      <td>1.163600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>1.173300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19600</td>\n",
              "      <td>1.158400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19700</td>\n",
              "      <td>1.161100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19800</td>\n",
              "      <td>1.163900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19900</td>\n",
              "      <td>1.165800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>1.163300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20100</td>\n",
              "      <td>1.157100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20200</td>\n",
              "      <td>1.155500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20300</td>\n",
              "      <td>1.156300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20400</td>\n",
              "      <td>1.154400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>1.151000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20600</td>\n",
              "      <td>1.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20700</td>\n",
              "      <td>1.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20800</td>\n",
              "      <td>1.158900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20900</td>\n",
              "      <td>1.156300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>1.161100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21100</td>\n",
              "      <td>1.155900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21200</td>\n",
              "      <td>1.159000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21300</td>\n",
              "      <td>1.164500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21400</td>\n",
              "      <td>1.146500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>1.147300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21600</td>\n",
              "      <td>1.163500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21700</td>\n",
              "      <td>1.152600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21800</td>\n",
              "      <td>1.151400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21900</td>\n",
              "      <td>1.135700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>1.150300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22100</td>\n",
              "      <td>1.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22200</td>\n",
              "      <td>1.144000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22300</td>\n",
              "      <td>1.151800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22400</td>\n",
              "      <td>1.161800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>1.145900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22600</td>\n",
              "      <td>1.144100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22700</td>\n",
              "      <td>1.149800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22800</td>\n",
              "      <td>1.145100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22900</td>\n",
              "      <td>1.152400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>1.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23100</td>\n",
              "      <td>1.145100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23200</td>\n",
              "      <td>1.152900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23300</td>\n",
              "      <td>1.163000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23400</td>\n",
              "      <td>1.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>1.150500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23600</td>\n",
              "      <td>1.151000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23700</td>\n",
              "      <td>1.156000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23800</td>\n",
              "      <td>1.148400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23900</td>\n",
              "      <td>1.147600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>1.152300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24100</td>\n",
              "      <td>1.151800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24200</td>\n",
              "      <td>1.147800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24300</td>\n",
              "      <td>1.148000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24400</td>\n",
              "      <td>1.152700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>1.154900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24600</td>\n",
              "      <td>1.139000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24700</td>\n",
              "      <td>1.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24800</td>\n",
              "      <td>1.138800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24900</td>\n",
              "      <td>1.142000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>1.124700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25100</td>\n",
              "      <td>1.127200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25200</td>\n",
              "      <td>1.127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25300</td>\n",
              "      <td>1.131300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25400</td>\n",
              "      <td>1.128900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>1.133500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25600</td>\n",
              "      <td>1.124800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25700</td>\n",
              "      <td>1.129900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25800</td>\n",
              "      <td>1.131200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25900</td>\n",
              "      <td>1.131400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>1.137000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26100</td>\n",
              "      <td>1.135700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26200</td>\n",
              "      <td>1.132900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26300</td>\n",
              "      <td>1.122600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26400</td>\n",
              "      <td>1.124300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>1.133000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26600</td>\n",
              "      <td>1.132100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26700</td>\n",
              "      <td>1.124200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26800</td>\n",
              "      <td>1.125800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26900</td>\n",
              "      <td>1.121100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>1.130800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27100</td>\n",
              "      <td>1.123600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27200</td>\n",
              "      <td>1.133900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27300</td>\n",
              "      <td>1.123400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27400</td>\n",
              "      <td>1.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>1.124400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27600</td>\n",
              "      <td>1.124000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27700</td>\n",
              "      <td>1.125100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27800</td>\n",
              "      <td>1.127300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27900</td>\n",
              "      <td>1.127900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>1.117600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28100</td>\n",
              "      <td>1.126200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28200</td>\n",
              "      <td>1.121500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28300</td>\n",
              "      <td>1.122900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28400</td>\n",
              "      <td>1.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>1.115900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28600</td>\n",
              "      <td>1.122200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28700</td>\n",
              "      <td>1.117100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28800</td>\n",
              "      <td>1.123700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28900</td>\n",
              "      <td>1.120800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>1.131200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29100</td>\n",
              "      <td>1.118000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29200</td>\n",
              "      <td>1.118300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29300</td>\n",
              "      <td>1.132800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29400</td>\n",
              "      <td>1.120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>1.119700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29600</td>\n",
              "      <td>1.125500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29700</td>\n",
              "      <td>1.130100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29800</td>\n",
              "      <td>1.130600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29900</td>\n",
              "      <td>1.131800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>1.113100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30100</td>\n",
              "      <td>1.126100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30200</td>\n",
              "      <td>1.121600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30300</td>\n",
              "      <td>1.128300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30400</td>\n",
              "      <td>1.125400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>1.125200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30600</td>\n",
              "      <td>1.127400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30700</td>\n",
              "      <td>1.114200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30800</td>\n",
              "      <td>1.120800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30900</td>\n",
              "      <td>1.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>1.128500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31100</td>\n",
              "      <td>1.132300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31200</td>\n",
              "      <td>1.115700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31300</td>\n",
              "      <td>1.125900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31400</td>\n",
              "      <td>1.121300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>1.124600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31600</td>\n",
              "      <td>1.115900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31700</td>\n",
              "      <td>1.127600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31800</td>\n",
              "      <td>1.122900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31900</td>\n",
              "      <td>1.117400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>1.132100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32100</td>\n",
              "      <td>1.121800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32200</td>\n",
              "      <td>1.119600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32300</td>\n",
              "      <td>1.127700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32400</td>\n",
              "      <td>1.117700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>1.129000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32600</td>\n",
              "      <td>1.117800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32700</td>\n",
              "      <td>1.124100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32800</td>\n",
              "      <td>1.120300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32900</td>\n",
              "      <td>1.120200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>1.120100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33100</td>\n",
              "      <td>1.113300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33200</td>\n",
              "      <td>1.128800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33300</td>\n",
              "      <td>1.128600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33400</td>\n",
              "      <td>1.118100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>1.121200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33600</td>\n",
              "      <td>1.121400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33700</td>\n",
              "      <td>1.116400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33800</td>\n",
              "      <td>1.116400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33900</td>\n",
              "      <td>1.114500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>1.128600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34100</td>\n",
              "      <td>1.113800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34200</td>\n",
              "      <td>1.115000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34300</td>\n",
              "      <td>1.124600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34400</td>\n",
              "      <td>1.106100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>1.112200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34600</td>\n",
              "      <td>1.121100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34700</td>\n",
              "      <td>1.116700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34800</td>\n",
              "      <td>1.116900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34900</td>\n",
              "      <td>1.116600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>1.108200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35100</td>\n",
              "      <td>1.115700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35200</td>\n",
              "      <td>1.126500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35300</td>\n",
              "      <td>1.104300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35400</td>\n",
              "      <td>1.124600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>1.121400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35600</td>\n",
              "      <td>1.112100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35700</td>\n",
              "      <td>1.117200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35800</td>\n",
              "      <td>1.110000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35900</td>\n",
              "      <td>1.111000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>1.124600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36100</td>\n",
              "      <td>1.125700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36200</td>\n",
              "      <td>1.117200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36300</td>\n",
              "      <td>1.114000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36400</td>\n",
              "      <td>1.113800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>1.112900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36600</td>\n",
              "      <td>1.109200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36700</td>\n",
              "      <td>1.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36800</td>\n",
              "      <td>1.113800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36900</td>\n",
              "      <td>1.112700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>1.118500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37100</td>\n",
              "      <td>1.113800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-1000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-1000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-1000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-1500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-1500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-1500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-2000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-2000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-2000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-1000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-2500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-2500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-2500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-1500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-3000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-3000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-3000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-2000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-3500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-3500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-3500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-2500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-4000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-4000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-4000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-3000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-4500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-4500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-4500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-3500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-5000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-5000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-5000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-4000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-5500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-5500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-5500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-4500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-6000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-6000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-6000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-5000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-6500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-6500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-6500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-5500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-7000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-7000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-7000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-6000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-7500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-7500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-7500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-6500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-8000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-8000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-8000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-7000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-8500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-8500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-8500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-7500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-9000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-9000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-9000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-8000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-9500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-9500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-9500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-8500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-10000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-10000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-10000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-9000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-10500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-10500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-10500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-9500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-11000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-11000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-11000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-10000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-11500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-11500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-11500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-10500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-12000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-12000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-12000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-11000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-12500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-12500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-12500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-11500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-13000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-13000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-13000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-12000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-13500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-13500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-13500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-12500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-14000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-14000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-14000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-13000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-14500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-14500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-14500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-13500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-15000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-15000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-15000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-14000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-15500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-15500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-15500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-14500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-16000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-16000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-16000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-15000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-16500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-16500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-16500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-15500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-17000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-17000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-17000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-16000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-17500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-17500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-17500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-16500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-18000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-18000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-18000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-17000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-18500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-18500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-18500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-17500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-19000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-19000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-19000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-18000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-19500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-19500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-19500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-18500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-20000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-20000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-20000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-19000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-20500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-20500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-20500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-19500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-21000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-21000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-21000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-20000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-21500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-21500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-21500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-20500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-22000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-22000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-22000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-21000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-22500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-22500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-22500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-21500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-23000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-23000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-23000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-22000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-23500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-23500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-23500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-22500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-24000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-24000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-24000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-23000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-24500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-24500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-24500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-23500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-25000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-25000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-25000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-24000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-25500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-25500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-25500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-24500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-26000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-26000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-26000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-25000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-26500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-26500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-26500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-25500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-27000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-27000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-27000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-26000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-27500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-27500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-27500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-26500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-28000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-28000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-28000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-27000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-28500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-28500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-28500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-27500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-29000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-29000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-29000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-28000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-29500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-29500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-29500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-28500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-30000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-30000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-30000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-29000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-30500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-30500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-30500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-29500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-31000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-31000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-31000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-30000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-31500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-31500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-31500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-30500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-32000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-32000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-32000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-31000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-32500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-32500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-32500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-31500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-33000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-33000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-33000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-32000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-33500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-33500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-33500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-32500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-34000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-34000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-34000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-33000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-34500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-34500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-34500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-33500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-35000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-35000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-35000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-34000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-35500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-35500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-35500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-34500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-36000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-36000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-36000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-35000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-36500\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-36500/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-36500/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-35500] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "Saving model checkpoint to ./gpt2_finetuned_therapy/checkpoint-37000\n",
            "Configuration saved in ./gpt2_finetuned_therapy/checkpoint-37000/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/checkpoint-37000/pytorch_model.bin\n",
            "Deleting older checkpoint [gpt2_finetuned_therapy/checkpoint-36000] due to args.save_total_limit\n",
            "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=37158, training_loss=1.2059365454284219, metrics={'train_runtime': 4419.0211, 'train_samples_per_second': 67.268, 'train_steps_per_second': 8.409, 'total_flos': 7.7671146848256e+16, 'train_loss': 1.2059365454284219, 'epoch': 3.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving weights"
      ],
      "metadata": {
        "id": "hbf1_7jrZG_Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNK3vBtySrUC",
        "outputId": "8f3e5d18-6730-4212-e9b8-1c13ba47c02a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./gpt2_finetuned_therapy\n",
            "Configuration saved in ./gpt2_finetuned_therapy/config.json\n",
            "Model weights saved in ./gpt2_finetuned_therapy/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model(\"./gpt2_finetuned_therapy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the trained model"
      ],
      "metadata": {
        "id": "PVBs4VGWbN2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_finetuned_therapy\")\n",
        "model = model.to(device)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_finetuned_therapy\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "c8pMVponbNBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model with some prompt"
      ],
      "metadata": {
        "id": "d0xKSAtHZMuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt, max_length=300, temperature=0.7, top_p=0.9):\n",
        "    # tokenizing the input\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # generating the model output\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "    # decoding the output\n",
        "    response = tokenizer.decode(output[0],\n",
        "                                skip_special_tokens=True)\n",
        "\n",
        "    response = response[len(prompt):].strip()\n",
        "    stop_words = [\"Patient:\", \"User:\", \"Charlie:\", \"Therapist:\", \"\\n\\n\", \"Human\"]\n",
        "\n",
        "    for token in stop_words:\n",
        "        if token in response:\n",
        "            response = response.split(token)[0].strip()  # taking everything before that word\n",
        "            break\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "Hkf9l1Z5ZD-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Patient: I'm feeling lonely\\nTherapist:\"\n",
        "resp = generate_response(prompt)\n",
        "print(resp)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T01:30:04.259970Z",
          "iopub.execute_input": "2025-04-04T01:30:04.260311Z",
          "iopub.status.idle": "2025-04-04T01:30:05.417591Z",
          "shell.execute_reply.started": "2025-04-04T01:30:04.260283Z",
          "shell.execute_reply": "2025-04-04T01:30:05.416615Z"
        },
        "id": "4qEK2jd5a9aU",
        "outputId": "68f1b6b9-5339-4ee6-c895-f304d5b2a8f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Hi Alex, I hope you're doing well. Loneliness can be a difficult emotion to deal with. Can we talk about it?\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interactive chatbot"
      ],
      "metadata": {
        "id": "e4YIqW4LbhDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_model(user_input, conversation_history=[]):\n",
        "    conversation_history.append(f\"Patient: {user_input}\")  # maintain conversation history for the conversation\n",
        "    full_prompt = \"\\n\".join(conversation_history) + \"\\nTherapist:\"  # prompt with conversation history\n",
        "\n",
        "    response = generate_response(full_prompt)\n",
        "\n",
        "    therapist_response = response.split(\"\\nPatient:\")[0].split(\"\\n\\n\")[0].strip() # Extract the therapist's response\n",
        "\n",
        "    conversation_history.append(f\"Therapist: {therapist_response}\") # appending therapist's response to history\n",
        "\n",
        "    return therapist_response, conversation_history"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T00:47:28.434727Z",
          "iopub.execute_input": "2025-04-04T00:47:28.435205Z",
          "iopub.status.idle": "2025-04-04T00:47:28.442390Z",
          "shell.execute_reply.started": "2025-04-04T00:47:28.435163Z",
          "shell.execute_reply": "2025-04-04T00:47:28.441435Z"
        },
        "id": "emYZ998ka9aU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "convo_history = []\n",
        "while True:\n",
        "    user_msg = input(\"Patient:\")\n",
        "    if user_msg == 'quit':  # command to exit the loop\n",
        "        break\n",
        "    response, convo_history = chat_with_model(user_msg, convo_history)\n",
        "    print(\"Therapist:\", response, end='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHeO2-JlcjSR",
        "outputId": "36ecc1f6-e89a-4f15-a758-deae789e9c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Patient: I've been feeling anxious about my addiction recovery.\n",
            "Therapist: Thank you for sharing, Alex. It's great to have someone here to talk to. What specifically has been causing your anxiety?\n",
            "Patient: I'm afraid I might start drinking again.\n",
            "Therapist: I understand that it's not a healthy coping mechanism, but remember, taking care of yourself is important. Can you tell me more about how your addiction affects your daily life and relationships?\n",
            "Patient: It disturbs my sleep and my wife is not talking to me\n",
            "Therapist: Addiction can strain our relationships in various ways. Have you tried any strategies to address the issue before? Perhaps seeking professional help or attending support groups?\n",
            "Patient: Yes, I got some prescriptions but they aren't helping\n",
            "Therapist: That's understandable. We need to find a way to manage the cravings while still supporting each other. Are there any triggers or situations that make your addiction more challenging?\n",
            "Patient: My friends, they influence me.\n",
            "Therapist: I notice that their influence on others plays a significant role in triggering my addiction. How does this affect your overall well-being? Does it make you feel anxious?\n",
            "Patient: quit\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Using perplexity measure"
      ],
      "metadata": {
        "id": "LXPB27cvxYFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "def perplexity(model, tokenizer, dataset, batch_size=4, max_length=512):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\",\n",
        "                           padding=True, truncation=True,\n",
        "                           max_length=max_length)\n",
        "        input_ids = inputs['input_ids'].to(model.device)\n",
        "        attention_mask = inputs['attention_mask'].to(model.device)\n",
        "\n",
        "        with torch.no_grad():  # calculating loss\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            total_tokens += input_ids.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Average Perplexity: {perplexity:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T02:10:06.602278Z",
          "iopub.execute_input": "2025-04-04T02:10:06.602584Z",
          "iopub.status.idle": "2025-04-04T02:10:06.608880Z",
          "shell.execute_reply.started": "2025-04-04T02:10:06.602540Z",
          "shell.execute_reply": "2025-04-04T02:10:06.607861Z"
        },
        "id": "kGuQRR3DxQdP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(model, tokenizer, dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T02:10:09.910825Z",
          "iopub.execute_input": "2025-04-04T02:10:09.911110Z",
          "iopub.status.idle": "2025-04-04T02:57:12.787134Z",
          "shell.execute_reply.started": "2025-04-04T02:10:09.911089Z",
          "shell.execute_reply": "2025-04-04T02:57:12.786351Z"
        },
        "id": "052ZaJ2rxQdP",
        "outputId": "61415817-c194-48f6-d59a-77a3d6b05fbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24772/24772 [47:02<00:00,  8.78it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Average Loss: 1.0435\nAverage Perplexity: 2.84\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning on 2nd Dataset: Empathetic Dialogues"
      ],
      "metadata": {
        "id": "frcqTTao5J2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset\n",
        "\n",
        "from huggingface"
      ],
      "metadata": {
        "id": "NJlOptvb5Y6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_dataset = load_dataset(\"facebook/empathetic_dialogues\", split=\"train\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T13:30:56.380103Z",
          "iopub.execute_input": "2025-04-29T13:30:56.380352Z",
          "iopub.status.idle": "2025-04-29T13:30:57.118072Z",
          "shell.execute_reply.started": "2025-04-29T13:30:56.380330Z",
          "shell.execute_reply": "2025-04-29T13:30:57.117421Z"
        },
        "id": "hIf1IY7X3whm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the dataset\n",
        "\n",
        "We preprocess the dataset in a way that is needed for the already fine-tuned model.\n",
        "\n",
        "We convert the utterances to \"Patient\" and \"Therapist\" like how we trained the previous model"
      ],
      "metadata": {
        "id": "h9rw9H-H5hl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_conversation(example):\n",
        "    user_msg = example[\"context\"] + \" \" + example[\"utterance\"]\n",
        "    response = example[\"selfeval\"] if \"selfeval\" in example and example[\"selfeval\"] else example[\"utterance\"]\n",
        "\n",
        "    formatted = f\"Patient: {user_msg.strip()}\\nTherapist: {example['utterance'].strip()}\"\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "emp_dataset_formatted = emp_dataset.map(convert_to_conversation)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T13:30:57.119741Z",
          "iopub.execute_input": "2025-04-29T13:30:57.120046Z",
          "iopub.status.idle": "2025-04-29T13:31:05.300486Z",
          "shell.execute_reply.started": "2025-04-29T13:30:57.120015Z",
          "shell.execute_reply": "2025-04-29T13:31:05.299493Z"
        },
        "id": "nnu-SEvi3wht",
        "outputId": "cff35fee-6f4d-4a20-dfc6-4d3f0df8853e",
        "colab": {
          "referenced_widgets": [
            "7e4532cfa0724ef697f6a8a1fd639552"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/76673 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e4532cfa0724ef697f6a8a1fd639552"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Then we tokenize the dataset with a max_length of 512"
      ],
      "metadata": {
        "id": "MkMst_5J59Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    tokens = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "emp_dataset_tokenized = emp_dataset_formatted.map(tokenize, batched=True, remove_columns=emp_dataset_formatted.column_names)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T13:31:05.301969Z",
          "iopub.execute_input": "2025-04-29T13:31:05.302195Z",
          "iopub.status.idle": "2025-04-29T13:31:52.654728Z",
          "shell.execute_reply.started": "2025-04-29T13:31:05.302176Z",
          "shell.execute_reply": "2025-04-29T13:31:52.653930Z"
        },
        "id": "6mNW3tlE3whu",
        "outputId": "568d50a4-0d64-45ac-edea-c34231620758",
        "colab": {
          "referenced_widgets": [
            "22a61470373841b5b5970969502cba3d"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/76673 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22a61470373841b5b5970969502cba3d"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "With similar arguments as the first fine-tuning, but with only 1 epoch and very less learning rate so that model will not be overwritten by this dataset."
      ],
      "metadata": {
        "id": "aSdpBajz6Daq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned_with_empathetic\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    learning_rate=1e-7,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",\n",
        "    evaluation_strategy=\"no\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=emp_dataset_tokenized,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T13:31:52.655490Z",
          "iopub.execute_input": "2025-04-29T13:31:52.655707Z",
          "iopub.status.idle": "2025-04-29T13:31:52.694555Z",
          "shell.execute_reply.started": "2025-04-29T13:31:52.655689Z",
          "shell.execute_reply": "2025-04-29T13:31:52.693598Z"
        },
        "id": "bH00o68U3whu",
        "outputId": "14b23259-ed2a-4fa8-9fc9-2cb23abdddce"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./gpt2_finetuned_therapy_plus_empathetic\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T13:31:52.695508Z",
          "iopub.execute_input": "2025-04-29T13:31:52.695823Z",
          "iopub.status.idle": "2025-04-29T15:21:46.061216Z",
          "shell.execute_reply.started": "2025-04-29T13:31:52.695790Z",
          "shell.execute_reply": "2025-04-29T15:21:46.060536Z"
        },
        "id": "XhbkMMAU3whu",
        "outputId": "3d48a36b-27e3-4359-9417-cf4d5afa117b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='19169' max='19169' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19169/19169 1:49:51, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.264400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.159800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.151600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.153000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.142700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.135200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.131900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.130800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.130400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.129100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.123500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.129000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.126900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.137700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.130500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.141600</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.125300</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.129300</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.130600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.127900</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.122400</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.122700</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.133500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.125900</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.128000</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.125800</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.129600</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.125000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.120500</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.133000</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.121400</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.120800</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.126100</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.125400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.122600</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.119000</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.126000</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.131200</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.116700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.121000</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.127300</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.131100</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.122300</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.116700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.122600</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.122000</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.117100</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.119000</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.119700</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.125900</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.118200</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.131600</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.120100</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.118600</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.113700</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.125800</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.127100</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.118800</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.116800</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.123700</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.117700</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.112500</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.133900</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.120900</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.115700</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.119100</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.121000</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.117700</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.124200</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.127400</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.123200</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.118900</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.116600</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.119400</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.117100</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.114100</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.119900</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.120200</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.118700</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.121300</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.127700</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.125200</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.124400</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.114900</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.117900</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.120200</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.124700</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.119200</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.125600</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.120100</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.122300</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.115900</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.123100</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.116000</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.117800</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.115400</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>0.116200</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>0.124300</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>0.119500</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.118500</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>0.120000</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>0.116200</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>0.123400</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>0.120300</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.124000</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>0.121600</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>0.122300</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>0.121300</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>0.116900</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.123400</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>0.115100</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>0.116200</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>0.115200</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>0.116500</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.120200</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>0.123400</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>0.112400</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>0.112000</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>0.116200</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.122500</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>0.113100</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>0.116200</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>0.122500</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>0.118800</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.121300</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>0.114800</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>0.112900</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>0.120600</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>0.113900</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.119300</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>0.119600</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>0.114800</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>0.116600</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>0.121500</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.113200</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>0.113400</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>0.119700</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>0.116200</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>0.119400</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.124200</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>0.119000</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>0.121400</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>0.112300</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>0.124400</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.117600</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>0.116000</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>0.109000</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>0.113000</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>0.114300</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.112100</td>\n    </tr>\n    <tr>\n      <td>15100</td>\n      <td>0.113000</td>\n    </tr>\n    <tr>\n      <td>15200</td>\n      <td>0.115700</td>\n    </tr>\n    <tr>\n      <td>15300</td>\n      <td>0.123400</td>\n    </tr>\n    <tr>\n      <td>15400</td>\n      <td>0.120600</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.118200</td>\n    </tr>\n    <tr>\n      <td>15600</td>\n      <td>0.111600</td>\n    </tr>\n    <tr>\n      <td>15700</td>\n      <td>0.116300</td>\n    </tr>\n    <tr>\n      <td>15800</td>\n      <td>0.113500</td>\n    </tr>\n    <tr>\n      <td>15900</td>\n      <td>0.115300</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.119000</td>\n    </tr>\n    <tr>\n      <td>16100</td>\n      <td>0.123400</td>\n    </tr>\n    <tr>\n      <td>16200</td>\n      <td>0.119000</td>\n    </tr>\n    <tr>\n      <td>16300</td>\n      <td>0.112300</td>\n    </tr>\n    <tr>\n      <td>16400</td>\n      <td>0.115000</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.113000</td>\n    </tr>\n    <tr>\n      <td>16600</td>\n      <td>0.113600</td>\n    </tr>\n    <tr>\n      <td>16700</td>\n      <td>0.116400</td>\n    </tr>\n    <tr>\n      <td>16800</td>\n      <td>0.116000</td>\n    </tr>\n    <tr>\n      <td>16900</td>\n      <td>0.114400</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.116400</td>\n    </tr>\n    <tr>\n      <td>17100</td>\n      <td>0.122700</td>\n    </tr>\n    <tr>\n      <td>17200</td>\n      <td>0.121700</td>\n    </tr>\n    <tr>\n      <td>17300</td>\n      <td>0.120600</td>\n    </tr>\n    <tr>\n      <td>17400</td>\n      <td>0.120800</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.113600</td>\n    </tr>\n    <tr>\n      <td>17600</td>\n      <td>0.117800</td>\n    </tr>\n    <tr>\n      <td>17700</td>\n      <td>0.114600</td>\n    </tr>\n    <tr>\n      <td>17800</td>\n      <td>0.117800</td>\n    </tr>\n    <tr>\n      <td>17900</td>\n      <td>0.112300</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.115300</td>\n    </tr>\n    <tr>\n      <td>18100</td>\n      <td>0.118300</td>\n    </tr>\n    <tr>\n      <td>18200</td>\n      <td>0.117400</td>\n    </tr>\n    <tr>\n      <td>18300</td>\n      <td>0.115000</td>\n    </tr>\n    <tr>\n      <td>18400</td>\n      <td>0.114700</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.121900</td>\n    </tr>\n    <tr>\n      <td>18600</td>\n      <td>0.119000</td>\n    </tr>\n    <tr>\n      <td>18700</td>\n      <td>0.115800</td>\n    </tr>\n    <tr>\n      <td>18800</td>\n      <td>0.117500</td>\n    </tr>\n    <tr>\n      <td>18900</td>\n      <td>0.118500</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.116500</td>\n    </tr>\n    <tr>\n      <td>19100</td>\n      <td>0.113600</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "NVHvNWVGG_8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convo_history = []\n",
        "while True:\n",
        "    user_msg = input(\"Patient:\")\n",
        "    if user_msg == 'quit':  # command to exit the loop\n",
        "        break\n",
        "    response, convo_history = chat_with_model(user_msg, convo_history)\n",
        "    print(\"Therapist:\", response, end='\\n')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-30T00:22:51.883356Z",
          "iopub.execute_input": "2025-04-30T00:22:51.883705Z",
          "iopub.status.idle": "2025-04-30T00:25:44.324788Z",
          "shell.execute_reply.started": "2025-04-30T00:22:51.883679Z",
          "shell.execute_reply": "2025-04-30T00:25:44.324072Z"
        },
        "id": "WH5jSuB6Gg7i",
        "outputId": "b0b96f6a-81a4-4fb6-a517-da77318a6b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient: I'm feeling really frustrated and sorry about my personal relationships.\n"
        },
        {
          "name": "stdout",
          "text": "Therapist: Hello Charlie, thank you for reaching out to me today. It takes courage to acknowledge our frustrations and apologize in a way that feels right for us all at the time of therapy sessions like this one together!\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient: It just seems like nothing is going right lately. I feel like I can't make any meaningful connections with the people in my life.\n"
        },
        {
          "name": "stdout",
          "text": "Therapist: That's understandable, Charlie - it can be disheartening when we invest so much effort into something and donâ€™t see immediate results. Can you tell me more about what specifically frustrates you? And how does your relationship relate back then too...\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient: I think part of it might be that I've been so caught up in my own life and problems that I haven't been fully present for the people around me.\n"
        },
        {
          "name": "stdout",
          "text": "Therapist: It sounds like you're aware there may have become distractions or unmet needs from others. Have you tried communicating your feelings openly without judgment or interruptions?\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient: No, I haven't. I guess I've been avoiding the conversation because I feel guilty about it.\n"
        },
        {
          "name": "stdout",
          "text": "Therapist: Avoiding conversations doesn often lead individuals to open up even deeper emotions they didn not expect. What do you believe would happen if you were to express yourself honestly again instead than holding back?\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient: quit\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT for summarization and long term memory\n",
        "\n",
        "This will act as a context maintainer for long conversation."
      ],
      "metadata": {
        "id": "xwtjlViPH_lv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O69sWI9hIzt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNeyaWW-Hl46",
        "outputId": "9006b6c3-92cd-425f-cf03-da05646eb5cb",
        "colab": {
          "referenced_widgets": [
            "43958d4b0fda491d95330c60f7a2f9ca",
            "7ea0e223212d4002ba086b5d2a82feb0",
            "b77b93d02c294100b6107ef0bf6b8857",
            "40cab05971dc4c4ebc8ae0273c45193d",
            "2d2c40a1f9b74522a83141dcb01d1b4e",
            "f929516ed2044d84990f3066ad2fb542"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43958d4b0fda491d95330c60f7a2f9ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ea0e223212d4002ba086b5d2a82feb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b77b93d02c294100b6107ef0bf6b8857",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40cab05971dc4c4ebc8ae0273c45193d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d2c40a1f9b74522a83141dcb01d1b4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f929516ed2044d84990f3066ad2fb542",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_log = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Patient: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    conversation_log.append(f\"Patient: {user_input}\")\n",
        "\n",
        "    # summarizing if conversation is longer than 3 exchanges\n",
        "    if len(conversation_log) >= 3:\n",
        "        raw_history = \"\\n\".join(conversation_log[-6:])  # last 6 conversations\n",
        "        summary = summarizer(raw_history, max_length=120, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
        "    else:\n",
        "        summary = \"\"\n",
        "    print(f\"Summary:( {summary} )\")\n",
        "    prompt = f\"Conversation Summary: {summary}\\nPatient: {user_input}\\nTherapist:\"\n",
        "\n",
        "    therapist_response = generate_response(prompt)\n",
        "    print(f\"Therapist: {therapist_response}\\n\")\n",
        "    conversation_log.append(f\"Therapist: {therapist_response}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-30T01:04:54.621936Z",
          "iopub.execute_input": "2025-04-30T01:04:54.622300Z",
          "iopub.status.idle": "2025-04-30T01:07:40.302637Z",
          "shell.execute_reply.started": "2025-04-30T01:04:54.622268Z",
          "shell.execute_reply": "2025-04-30T01:07:40.301671Z"
        },
        "id": "seOvqJFmPOgo",
        "outputId": "6b9cc135-cea9-4d2e-9aaf-cb846247797b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient:  I'm feeling disappointed with my job.\n"
        },
        {
          "name": "stdout",
          "text": "Summary:(  )\nTherapist: Hi Charlie, thank you for reaching out. Can you tell me more about what's been going on? What specifically is causing you to feel this way at work and how it's impacting your overall well-being?\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient:  I've been working at the same company for years, but I feel stuck. There's no room for growth or advancement.\n"
        },
        {
          "name": "stderr",
          "text": "Your max_length is set to 120, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Summary:( Patient: I've been working at the same company for years, but I feel stuck. There's no room for growth or advancement. Patient: I'm feeling disappointed with my job. )\nTherapist: I understand that feeling of disappointment. It's tough when we put effort into our work and it doesn't yield the desired results.\nHumanDefinitely not. The lack of progress is making me question my abilities. How can I find a new hobby to help me overcome this?\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient:  I feel like my talents and skills aren't being utilized. I'm not given the chance to take on projects that truly interest me.\n"
        },
        {
          "name": "stdout",
          "text": "Summary:( Patient: I've been working at the same company for years, but I feel stuck. Therapist: I understand that feeling of disappointment. How can I find a new hobby to help me overcome this? )\nTherapist: It's disheartening when we donâ€™t see immediate results in our careers. What are some hobbies you have considered exploring or trying out? Perhaps something creative outside your current job?\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient:  I have, but it hasn't led to any significant changes. I feel like my voice isn't being heard.\n"
        },
        {
          "name": "stdout",
          "text": "Summary:( Patient: I've been working at the same company for years, but I feel stuck. There's no room for growth or advancement. Therapist: It's disheartening when we don't see immediate results in our careers. )\nTherapist: That must be really frustrating for you. How does this lack of progress affect your overall well-being? Are there specific areas where you struggle the most with growth and advancement?\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient:  I'm afraid of change and starting over somewhere else. I also worry about financial stability if I were to leave my current job.\n"
        },
        {
          "name": "stdout",
          "text": "Summary:( Patient: I feel like my talents and skills aren't being utilized. Therapist: I understand that feeling of disappointment. It's tough when we put effort into our work and it doesn't yield the desired results. )\nTherapist: Change can be intimidating, but remember that growth often happens outside your comfort zone. Let me ask you this, what is one thing specifically making you unhappy in your career? Is there a particular area where you struggle the most with growth or finding fulfillment?\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient:  I've thought about it, but I'm not sure where to start. I feel overwhelmed by the thought of making such a big decision.\n"
        },
        {
          "name": "stdout",
          "text": "Summary:( It's disheartening when we don't see immediate results in our careers. Are there specific areas where you struggle the most with growth and advancement? I'm afraid of change and starting over somewhere else. )\nTherapist: Change can be daunting, especially when it involves leaving behind familiar routines and responsibilities. What specifically makes you hesitant about taking that leap? And how do you envision your career changing once you make this choice finally made?\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient:  I'd like a job that allows me to use my creativity, offers room for growth, and where I feel valued and appreciated.\n"
        },
        {
          "name": "stdout",
          "text": "Summary:( Patient: I'm afraid of change and starting over somewhere else. I worry about financial stability if I were to leave my current job. I'd like a job that allows me to use my creativity. )\nTherapist: It's understandable to have concerns when faced with uncertainty. Change can be intimidating, but it also opens doors to new possibilities. What specifically makes you fearful of leaving your current job?\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Patient:  quit\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation using BERT Score"
      ],
      "metadata": {
        "id": "wi87hBtvlFrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEdaHsrik8Tn",
        "outputId": "40ddc1d6-a181-42e4-f54a-962d445b62df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install nltk bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "759dbabcac634acb8983b9acd29b0015",
            "f526129d54da4c78ad6e0e63963f3b2f",
            "05a992fa892c4628ba3f94a8e9cfc4d9",
            "b1d2f8cb89024362bede575113c5aedf",
            "25587b9ad0ca411886880fa4a63e3866",
            "565158df93874013bb2d5d8c0571d4e4",
            "4416b2bd46874d4cb455fe7b3f51ca17",
            "8aa6eca577274a31bfb46fb3c8042b15",
            "86c44c96e86f4fd89b3b72d927854f4e",
            "b9d55dcedf554b6991a78450c16709cd",
            "277bc6b1d6ae4eee99848aa1b0b93a58",
            "d18b734097fc420f976a591f14142279",
            "9a182aaf70684e01869784b198f7d623",
            "13ac4217e2e94490a66e081bbca832f2",
            "bd021b5afeb84918b397593923a22ba9",
            "facbafd72e6f4ea2b2342ce79b327f76",
            "0a93050f62cd415c8b4d774e4ea704be",
            "ac4a6237f0f1442692084ca05b6602bd",
            "a2f8d50612a5429680a6145889119242",
            "8973e054c6da441a8b0a9050a71f4900",
            "e1e8846b871547bcb71007ca85ea850f",
            "9757ad64f790427d8c5e470ba5a464eb"
          ]
        },
        "id": "NmXd9mSEk8Tn",
        "outputId": "95dfc15e-9889-435a-ee0c-38d1c7337c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " BLEU Score: 0.0074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "759dbabcac634acb8983b9acd29b0015"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d18b734097fc420f976a591f14142279"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done in 10.41 seconds, 0.58 sentences/sec\n",
            "BERTScore F1 (avg): 0.8948\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from bert_score import score as bert_score\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "reference_texts = [\n",
        "    \"I can see why you'd feel disappointed. Putting in effort without recognition can be really discouraging.\",\n",
        "    \"That sounds really tough. Have you thought about creative hobbies or new interests outside of work that could bring you joy?\",\n",
        "    \"It's frustrating when you feel stuck. Can you share how this is affecting your mood or overall motivation?\",\n",
        "    \"Itâ€™s natural to be afraid of change. What aspects of your current job feel most unfulfilling or misaligned with your goals?\",\n",
        "    \"Leaving something familiar can be overwhelming. Whatâ€™s holding you back the most â€” the uncertainty, finances, or something else?\",\n",
        "    \"It's okay to feel nervous. But trying something new might bring the growth and appreciation youâ€™re looking for. What would that ideal job look like?\"\n",
        "]\n",
        "\n",
        "# collecting the generated text from above\n",
        "generated_texts = [\n",
        "    \"I understand that feeling of disappointment. It's tough when we put effort into our work and it doesn't yield the desired results.\",\n",
        "    \"It's disheartening when we donâ€™t see immediate results in our careers. What are some hobbies you have considered exploring or trying out? Perhaps something creative outside your current job?\",\n",
        "    \"That must be really frustrating for you. How does this lack of progress affect your overall well-being? Are there specific areas where you struggle the most with growth and advancement?\",\n",
        "    \"Change can be intimidating, but remember that growth often happens outside your comfort zone. Let me ask you this, what is one thing specifically making you unhappy in your career? Is there a particular area where you struggle the most with growth or finding fulfillment?\",\n",
        "    \"Change can be daunting, especially when it involves leaving behind familiar routines and responsibilities. What specifically makes you hesitant about taking that leap? And how do you envision your career changing once you make this choice finally made?\",\n",
        "    \"It's understandable to have concerns when faced with uncertainty. Change can be intimidating, but it also opens doors to new possibilities. What specifically makes you fearful of leaving your current job?\"\n",
        "]\n",
        "\n",
        "references_bleu = [[ref.split()] for ref in reference_texts]\n",
        "predictions_bleu = [pred.split() for pred in generated_texts]\n",
        "\n",
        "smoothie = SmoothingFunction().method4\n",
        "bleu = corpus_bleu(references_bleu, predictions_bleu, smoothing_function=smoothie)\n",
        "\n",
        "print(f\"\\n BLEU Score: {bleu:.4f}\")\n",
        "\n",
        "P, R, F1 = bert_score(generated_texts, reference_texts, lang=\"en\", verbose=True)\n",
        "print(f\"BERTScore F1 (avg): {F1.mean().item():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 7182840,
          "datasetId": 4088932,
          "sourceId": 7095001,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "sreeramm",
      "language": "python",
      "name": "sreeramm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}